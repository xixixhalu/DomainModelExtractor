As a I can both extract and visualize player value biographical data player stat injury salary so that I can build roster for the next season and make informed decis affecting the 
As a GM I can extract data related to finance and oper including staff and player salary revenue cost accounting 
As a GM my data can seamlessly integrate with the SporTech system in a standard compliant manner READ The system shall communicate in PostgreSQL 
Webcrawler must be able to visit public site extract simple structured relational data in table organized into row and column generate PostgreSQL INSERT statement on a configurable schedule and check for conflict within the existing SQL DB 
The crawler must be able to perform the above function for all common form of relational data integ decimal number date string boolean 
If there is no data existing for the unique identifier for the row within the table execute the insert statement for the dataset 
If there is a conflict there is already information in the row for the designated table the application must be able to generate and execute a PostgreSQL UPDATE statement for the crawled dataset 
As a SporTech contractor I can add delete update the specific website visited field to capture from the website and frequency of crawler refresh for each specified website 
Webcrawler app must function in a persistent manner if we set it using the above GUI it should run without intervention if there are no error 
The system must be able to function on a major OS READ Some well known distro of Linux like Ubuntu or Debian 
The crawler should be able identify and ingest into a NoSQL data store unstructured data social media information picture videos etc on a configurable schedule and check for conflict within the existing data store 
All webcrawler functionality from the relational data base ingest should be mirrored for unstructured data ingest Everything the crawler can do for structured data it should do for unstructured data 
With this condition in place as a general manger I can monitor from key source in a continuous manner on demand 
The crawler shall self correct simple error it encounters during the SQL ingest process 
The crawler should have some kind of visualization tool which allows engin to view a snapshot of the interact between the crawler datasourc and STBI database 
The crawler should search for new datasourc automatically it should be able to learn the type of data that are commonly ingested and then go look for new one of high quality 
The system shall be a management GUI As a customer I could manage the tool from anywhere without the need to worry about the SporTech system having to launch from a program on a desktop or server 
The system shall serve as the nerve center for the STBI ecosystem It can ingest all kind of structured data both automatically and on demand 
There should be a portal that allows the user to drag and drop information in many different format excel CSV web and then the crawler should organize sort and identify the datatype and then ingest into the PostgreSQL DB 
This would remove most or all of the work that a human would have to do to interact with the data before it is ready to be analyzed and visualized 
The system shall have two portal or web interfaces One would be for the SporTech contractor and would have many more advanced feature and controls The second would be a simple portal so that the customer could ingest their data and see the impact in their immediately 
As a customer I can import existing data from an excel sheet by dragging and dropping it into STBI so that I can data to my current model and see the change 
the crawler should organize sort and identify the datatype and then ingest into the DB 
As a SporTech contractor I can the player data as the season progresses 
As a soccer coach I can configure the schedule for the crawler to run at predefined time 
The web crawler shall gather video from the page being crawled and ingest into STBI as is so that the coach and fan is able to watch the relevant video 
The webcraweler shall gather head shot of player from the biography page on the website being crawled so that the player picture can be shown on the report being generated 
As a a soccer coach I can specify which websites the crawler should visit via the website GUI 
The web crawler shall crawl Youtube to gather video of specific player 
The web crawler shall get comment name and number of member likes from specified Facebook page 
The web crawler shall get number of follow the comment and the number of retweet for a specified twitter account 
The web crawler shall gather instagram picture number of like and the comment from particular instagram account 
The web crawler shall gather player information from the website in the website list 
The web crawler shall gather team information from the website in the website list 
The crawler shall ingest crawled data into PostgreSQL database 
