As a GM/coach, I can both extract and visualize player value (biographical data,player stats,injuries,salary) so that I can build roster for the next season and make informed decisions affecting the bottom-line.
As a GM, I can extract data related to finances and operations, including staff and player salaries, revenue, cost accounting.
As a GM, my data can seamlessly integrate with the SporTech B.I. system in a standard compliant manner. (READ: The system shall communicate in PostgreSQL.).
Webcrawler must be able to visit public sites, extract simple structured relational data (in tables organized into rows and columns), generate PostgreSQL INSERT statements on a configurable schedule, and check for conflicts within the existing SQL DB.
The crawler must be able to perform the above functions for all common forms of relational data (integers, decimal numbers, dates, strings, booleans etc.)
If there is no data existing for the unique identifier for the row within the table, execute the insert statements for the dataset.
If there is a conflict (there is already information in the row for the designated table) the application must be able to generate and execute  a PostgreSQL UPDATE statement for the "crawled" dataset.
As a SporTech B.I. contractor, I can  add,delete,update the specific websites visited,  fields to capture from the website and frequency of crawler refreshes for each specified website.
Webcrawler app must function in a persistent manner - if we set it using the above GUI, it should run without intervention if there are no errors.
The system must be able to function on a major open-source OS (READ: Some well known distro of Linux like Ubuntu or Debian).
The crawler should be able identify and ingest into a Hadoop/Hive/Pig NoSQL data store, unstructured data (social media information, pictures, videos etc), on a configurable schedule, and check for conflicts within the existing data store.
All webcrawler functionality from the relational data base ingest should be mirrored for unstructured data ingest.  (Everything the crawler can do for structured data it should do for unstructured data).
With this condition in place, as a general manger, I can monitor real-time twitter/facebook/instagram feeds/reaction from key sources in a continuous manner and/or on demand.
The crawler shall self correct simple errors it encounters during the SQL ingest process.
The crawler should have some kind of visualization tool which allows engineers to view a snapshot of the interactions between the crawler, datasources and STBI databases.
The crawler should search for new datasources automatically (it should be able to learn the types of data that are commonly ingested and then go look for new ones of high quality).
The system shall be a web-based management GUI. As a customer, I could manage the tool from anywhere without the need to worry about the SporTech B.I. system having to launch from a program on a desktop or server.
The system shall serve as the "nerve center" for the STBI ecosystem.  It can ingest all kinds of structured data, both automatically and on demand.
There should be a portal that allows the user to drag and drop information in many different formats (excel, CSV, web etc.) and then the crawler should organize, sort and identify the datatype and then ingest into the PostgreSQL DB.
This would remove most (or all) of the work that a human would have to do to interact with the data before it is ready to be analyzed and visualized.
The system shall have two portals (or web interfaces). One would be for the SporTech B.I contractors and would have many more advanced features and controls.  The second would be a simple, web-based, customer-facing portal so that the customer could ingest their data "self-service" and see the impact in their apps/dashboards immediately.
As a customer, I can import existing data from an excel sheet by dragging and dropping it into STBI so that I can update/add data to my current model and see the changes.
the crawler should organize, sort and identify the datatype and then ingest into the DB.
As a SporTech B.I contractor, I can update/revise the player data as the season progresses.
As a soccer coach, I can configure the schedule for the crawler to run at predefined times.
The web crawler shall gather videos from the pages being crawled and ingest into STBI as is so that the coach and fans is able to watch the relevant videos PA.
The webcrawler shall gather head shots of players from the biography page on the website being crawled so that the player's picture can be shown on the report being generated.
As a a soccer coach, I can specify which websites the crawler should visit via the website GUI.
The web crawler shall crawl Youtube to gather videos of specific players.
The web crawler shall get comments, name and number of members, likes from specified Facebook pages.
The web crawler shall get number of followers, the comments and the number of retweets for a specified twitter account.
The web crawler shall gather instagram pictures, number of likes and the comments from particular instagram account.
The web crawler shall gather player information from the websites in the website list.
The web crawler shall gather team information from the websites in the website list.
The crawler shall ingest crawled data into PostgreSQL database.
