Webcrawler must be able to visit public Sites , extract simple structured relational Data , generate PostgreSQLINSERTStatements on a configurable Schedule , and check for Conflicts within the existing SQLDB .
The Crawler must be able to perform the above Functions for all common Forms of relational Data
If there is no Data existing for the unique Identifier for the Row within the Table , execute the InsertStatements for the Dataset .
If there is a Conflict the Application must be able to generate and execute a PostgreSQLUPDATEStatement for the `` crawled '' Dataset .
WebcrawlerApp must function in a persistent Manner - if we set it using the above GUI , it should run without Intervention if there are no Errors .
The System must be able to function on a major open-source OS .
The Crawler should be able identify and ingest into a HadoopDataStore , unstructured Data , on a configurable Schedule , and check for Conflicts within the existing DataStore .
The Crawler should be able identify and ingest into a HiveDataStore , unstructured Data , on a configurable Schedule , and check for Conflicts within the existing DataStore .
The Crawler should be able identify and ingest into a PigNoSQLDataStore , unstructured Data , on a configurable Schedule , and check Check for Conflicts within the existing DataStore .
All WebcrawlerFunctionality from the relational DataBaseIngest should be mirrored for unstructured DataIngest . .
The Crawler shall self correct simple Errors it encounters during the SQLIngestProcess .
The Crawler should have some kind of VisualizationTool which allows Engineers to view a Snapshot of the Interactions between the Crawler , Datasources and STBI Databases .
The Crawler should search for new Datasources automatically .
There should be a Portal that allows the User to drag and drop Information in many different Formats and then the Crawler should organize , sort and identify the Datatype and then ingest into the PostgreSQLDB .
This would remove most of the Work that a Human would have to do to interact with the Data before it is ready to be analyzed and visualized .
The System shall have two portals . One would be for the SporTechB.IContractors and would have many more advanced Features and Controls . The Second would be a simple , web-based , customer-facing Portal so that the Customer could ingest their Data `` self-service '' and see the Impact in their Apps immediately .
The System shall have two portals . One would be for the SporTechB.IContractors and would have many more advanced Features and Controls . The Second would be a simple , web-based , customer-facing Portal so that the Customer could ingest their Data `` self-service '' and see the Impact in their Dashboards immediately .
the Crawler should organize , sort and identify the Datatype and then ingest into the DB .
The WebCrawler shall gather Videos from the Pages being crawled and ingest into STBI as is so that the Coach and Fans is able to watch the relevant VideosPA . .
The Webcrawler shall gather HeadShots of Players from the BiographyPage on the Website being crawled so that the Player 's Picture can be shown on the Report being generated .
The WebCrawler shall crawl Youtube to gather Videos of specific Players .
The WebCrawler shall get Comments , Name and Number of members , likes from specified FacebookPages .
The WebCrawler shall get Number of Followers , the Comments and the Number of Retweets for a specified TwitterAccount .
The WebCrawler shall gather InstagramPictures , Number of Likes and the Comments from particular InstagramAccount .
The WebCrawler shall gather PlayerInformation from the Websites in the WebsiteList .
The WebCrawler shall gather TeamInformation from the Websites in the WebsiteList .
The Crawler shall ingest crawled Data into PostgreSQLDatabase .
